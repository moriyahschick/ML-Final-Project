{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content of this notebook\n",
    "\n",
    "Two regression problems: predict number of crimes per week and average number of crimes per day \n",
    "Preparation code does both. Some steps are the same where otheres are different\n",
    "weekly specific variables are designated with prefix _w and daily average with _d\n",
    "Regression algoritms are the same and will be executed twice by changing input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nesessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as math \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import calendar\n",
    "# from datetime import datetime\n",
    "\n",
    "from astral.sun import sun\n",
    "from astral import LocationInfo\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from shapely.geometry.point import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from shapely import wkt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('/Volumes/CUNYGC/GC/CIS74020_ML/Final Project/Data/NYPD_Complaint_All_Clean.csv', sep=\",\")\n",
    "\n",
    "subway = pd.read_csv('/Volumes/CUNYGC/GC/CIS74020_ML/Final Project/Data/SUBWAY_ENTRANCE.csv')\n",
    "\n",
    "nynta = pd.read_csv ('/Volumes/CUNYGC/GC/CIS74020_ML/Final Project/Data/nynta.csv')\n",
    "\n",
    "nyc_pop = pd.read_csv('/Volumes/CUNYGC/GC/CIS74020_ML/Final Project/Data/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv')\n",
    "nyc_pop2010 = nyc_pop[nyc_pop['Year'] == 2010]\n",
    "\n",
    "holidays = pd.read_csv ('/Volumes/CUNYGC/GC/CIS74020_ML/Final Project/Data/US_Holiday_Dates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== Retrieve holidays and use nubmer of hilidays per week\n",
    "holidays['Date'] = pd.to_datetime(holidays['Date'], format='%Y-%m-%d', errors = 'coerce')\n",
    "holidays['WEEK'] = holidays['Date'].dt.isocalendar().week \n",
    "holidays['YEAR'] = holidays['Date'].dt.year\n",
    "holidays_per_week= holidays.groupby(['YEAR' , 'WEEK']).size().reset_index(name='H_COUNTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subways per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== Determine number of subway stations per census tract region\n",
    "#===================================================================\n",
    "nynta['geometry'] = nynta['the_geom'].apply(wkt.loads)\n",
    "nynta_geo_df = gpd.GeoDataFrame(nynta, geometry='geometry')\n",
    "# nynta_geo_df.info()\n",
    "nynta_geo_df['geometry']\n",
    "\n",
    "poly = nynta_geo_df['geometry'] \n",
    "\n",
    "#===== Retrieve subway entrances geo location and count # of stations per each census tract\n",
    "subway['geometry'] = subway['the_geom'].apply(wkt.loads)\n",
    "subway_geo_df = gpd.GeoDataFrame(subway, geometry='geometry')\n",
    "\n",
    "def subwayST(x):\n",
    "    point = Point(x['geometry'].x, x['geometry'].y)\n",
    "    if sum(poly.contains(point)) == 1:\n",
    "        code = nynta_geo_df[poly.contains(point)]['NTACode'].item()\n",
    "    else:\n",
    "        code = 'NA'\n",
    "    return (code)\n",
    "    \n",
    "subway_geo_df['NTACode'] = subway_geo_df.apply(subwayST, axis=1) \n",
    "\n",
    "subway_per_tract =  subway_geo_df.groupby(['NTACode']).size().reset_index(name='SUB_CNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define counts (group by) for two regression problems: Weekly totals and daily average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define crime category for prediction\n",
    "#---------------------------------------------\n",
    "#++++++++++++++ Weeky total set up   +++++++++++++++++\n",
    "# ofns_list = ['BURGLARY', 'INTOXICATED & IMPAIRED DRIVING', 'DANGEROUS DRUGS']\n",
    "ofns_list = ['PETIT LARCENY', 'GRAND LARCENY']\n",
    "# ofns_list = ['PETIT LARCENY', 'HARRASSMENT 2', 'ASSAULT 3 & RELATED OFFENSES', \n",
    "#               'CRIMINAL MISCHIEF & RELATED OF', 'OFF. AGNST PUB ORD SENSBLTY &', 'FELONY ASSAULT']\n",
    "df_all_sel = df_all[df_all['OFNS_DESC'].isin(ofns_list) ]\n",
    "\n",
    "#Define grouping for aggregation\n",
    "df_count_w = df_all_sel.groupby(['NTACode', 'YEAR', 'WEEK']).size().reset_index(name='counts') #per day\n",
    "\n",
    "#++++++++++++++ Daily average set up   +++++++++++++++++\n",
    "#Define grouping for aggregation\n",
    "df_count_d = df_all_sel.groupby(['NTACode', 'WEEK', 'DAY_OF_WEEK']).size().reset_index(name='counts') #per day\n",
    "num_of_years = len(df_all_sel.YEAR.unique())\n",
    "df_count_d.counts = df_count_d.counts/num_of_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add zero count records for the days or weeks where groupby resulted in no rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nta_codes = pd.DataFrame(nynta.NTACode.unique())\n",
    "all_nta_codes.columns = ['NTACode']\n",
    "\n",
    "date_range = pd.date_range(start='1/1/2006', end='12/31/2020', freq='D')\n",
    "date_range = pd.DatetimeIndex.to_frame(date_range).reset_index()\n",
    "\n",
    "date_range['YEAR'] = date_range[0].dt.year\n",
    "date_range['WEEK'] = date_range[0].dt.isocalendar().week\n",
    "date_range['DAY_OF_WEEK'] = date_range[0].dt.dayofweek\n",
    "date_range['ZERO_COUNT'] = 0\n",
    "\n",
    "# date_range=date_range[['YEAR', 'WEEK','ZERO_COUNT']].sort_values(by=['YEAR', 'WEEK', 'ZERO_COUNT'], ascending=True) \n",
    "\n",
    "#++++++++++++++ Weeky total set up   +++++++++++++++++\n",
    "date_range_w = date_range.groupby(['YEAR', 'WEEK', 'ZERO_COUNT']).size().reset_index()\n",
    "date_range_w = date_range_w[['YEAR', 'WEEK', 'ZERO_COUNT']]\n",
    "\n",
    "all_nta_codes['key'] = 0\n",
    "date_range_w['key'] = 0\n",
    "default_count_df_w  = all_nta_codes.merge(date_range_w, how='outer')\n",
    "default_count_df_w = default_count_df_w.drop('key', axis=1)\n",
    "\n",
    "default_count_df_w = default_count_df_w.merge(df_count_w,how='outer', on=['NTACode','YEAR', 'WEEK'], indicator=True)\n",
    "\n",
    "default_count_df_w = default_count_df_w[default_count_df_w['_merge'] == 'left_only'] \n",
    "default_count_df_w = default_count_df_w[['NTACode','YEAR', 'WEEK', 'ZERO_COUNT']]\n",
    "\n",
    "default_count_df_w = pd.concat([df_count_w, default_count_df_w], axis=0) \n",
    "\n",
    "\n",
    "default_count_df_w['counts'] = default_count_df_w['counts'].fillna(0)\n",
    "default_count_df_w = default_count_df_w[['NTACode','YEAR', 'WEEK', 'counts']]\n",
    "\n",
    "#++++++++++++++ Daily average set up   +++++++++++++++++\n",
    "date_range_d = date_range.groupby(['WEEK', 'DAY_OF_WEEK', 'ZERO_COUNT']).size().reset_index()\n",
    "date_range_d = date_range_d[['WEEK', 'DAY_OF_WEEK', 'ZERO_COUNT']]\n",
    "\n",
    "all_nta_codes['key'] = 0\n",
    "date_range_d['key'] = 0\n",
    "default_count_df_d  = all_nta_codes.merge(date_range_d, how='outer')\n",
    "default_count_df_d = default_count_df_d.drop('key', axis=1)\n",
    "\n",
    "default_count_df_d = default_count_df_d.merge(df_count_d,how='outer', on=['NTACode','DAY_OF_WEEK', 'WEEK'], indicator=True)\n",
    "\n",
    "default_count_df_d = default_count_df_d[default_count_df_d['_merge'] == 'left_only'] \n",
    "default_count_df_d = default_count_df_d[['NTACode','WEEK', 'DAY_OF_WEEK', 'ZERO_COUNT']]\n",
    "\n",
    "default_count_df_d = pd.concat([df_count_d, default_count_df_d], axis=0) \n",
    "\n",
    "default_count_df_d['counts'] = default_count_df_d['counts'].fillna(0)\n",
    "default_count_df_d = default_count_df_d[['NTACode','WEEK', 'DAY_OF_WEEK', 'counts']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge data to sensus tract population data, subsway counts, holiday counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from complaints to population and subsway stations\n",
    "#--------------------------------------------------------------\n",
    "#++++++++++++++ Weeky total set up   +++++++++++++++++\n",
    "df_count1_w  = default_count_df_w.merge(nyc_pop2010,left_on='NTACode',right_on='NTA Code').merge(nynta_geo_df,on='NTACode') \\\n",
    "    .merge(subway_per_tract,how=\"outer\",on='NTACode') \\\n",
    "    .merge(holidays_per_week,how=\"left\",left_on=('YEAR', 'WEEK'),right_on=('YEAR', 'WEEK'))\n",
    "   \n",
    "df_count1_w['SUB_CNT'] = df_count1_w['SUB_CNT'].fillna(0)   #if no subsway station found in tract - set to 0 intead of null\n",
    "df_count1_w['H_COUNTS'] = df_count1_w['H_COUNTS'].fillna(0)   #if no holidays found for a week - set to 0 intead of null\n",
    "\n",
    "# Retieve only needed columns\n",
    "df_count2_w = df_count1_w[['NTACode', 'NTAName', 'YEAR', 'WEEK','BoroName', 'Shape_Leng', 'Population',\n",
    "                            'Shape_Area', 'geometry','SUB_CNT', 'H_COUNTS', 'counts']]\n",
    "                            \n",
    "df_count2_w['POP_DENS'] = 1000* df_count2_w['Population']/df_count2_w['Shape_Area'] \n",
    "df_count2_w = df_count2_w[[ 'NTACode','POP_DENS', 'SUB_CNT', 'YEAR', 'WEEK', 'H_COUNTS', 'counts']]\n",
    "\n",
    "#++++++++++++++ Daily average set up   +++++++++++++++++\n",
    "df_count1_d = default_count_df_d.merge(nyc_pop2010,left_on='NTACode',right_on='NTA Code').merge(nynta_geo_df,on='NTACode') \\\n",
    "    .merge(subway_per_tract,how=\"outer\",on='NTACode') \n",
    " \n",
    "df_count1_d['SUB_CNT'] = df_count1_d['SUB_CNT'].fillna(0)   #if no subsway station found in tract - set to 0 intead of null\n",
    "\n",
    "# Retieve only needed columns\n",
    "df_count2_d = df_count1_d[['NTACode', 'NTAName', 'WEEK', 'DAY_OF_WEEK', 'BoroName', 'Shape_Leng', 'Population',\n",
    "                       'Shape_Area', 'geometry','SUB_CNT', 'counts']]\n",
    "                            \n",
    "df_count2_d['POP_DENS'] = 1000* df_count2_d['Population']/df_count2_d['Shape_Area'] \n",
    "df_count2_d = df_count2_d[[ 'NTACode','POP_DENS', 'SUB_CNT', 'WEEK', 'DAY_OF_WEEK', 'counts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummy variables and separate predictors and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "#== Create dummy variables, scale, and extract target variable  =========\n",
    "#========================================================================\n",
    "#++++++++++++++ Weeky total set up   +++++++++++++++++\n",
    "df_count2_dummy_w = pd.get_dummies(df_count2_w, columns=[ 'NTACode', 'YEAR', 'WEEK'])\n",
    "df_count2_dummy_w.POP_DENS = ( df_count2_dummy_w.POP_DENS - df_count2_dummy_w.POP_DENS.mean() ) / df_count2_dummy_w.POP_DENS.std()\n",
    "df_count2_dummy_w.SUB_CNT = ( df_count2_dummy_w.SUB_CNT - df_count2_dummy_w.SUB_CNT.mean() ) / df_count2_dummy_w.SUB_CNT.std()\n",
    "df_count2_dummy_w.H_COUNTS = ( df_count2_dummy_w.H_COUNTS - df_count2_dummy_w.H_COUNTS.mean() ) / df_count2_dummy_w.H_COUNTS.std()\n",
    "\n",
    "X_w = np.asarray (df_count2_dummy_w.drop('counts', axis=1) )\n",
    "y_w = np.asarray (df_count2_dummy_w['counts'])\n",
    "\n",
    "#++++++++++++++ Daily average set up   +++++++++++++++++\n",
    "df_count2_dummy_d = pd.get_dummies(df_count2_d, columns=[ 'NTACode', 'WEEK', 'DAY_OF_WEEK'])\n",
    "\n",
    "df_count2_dummy_d.POP_DENS = ( df_count2_dummy_d.POP_DENS - df_count2_dummy_d.POP_DENS.mean() ) / df_count2_dummy_d.POP_DENS.std()\n",
    "df_count2_dummy_d.SUB_CNT = ( df_count2_dummy_d.SUB_CNT - df_count2_dummy_d.SUB_CNT.mean() ) / df_count2_dummy_d.SUB_CNT.std()\n",
    "\n",
    "X_d = np.asarray (df_count2_dummy_d.drop('counts', axis=1) )\n",
    "y_d = np.asarray (df_count2_dummy_d['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idenify features correlated to the target for weekly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          1.000000\n",
      "SUB_CNT         0.778283\n",
      "NTACode_MN17    0.626360\n",
      "NTACode_MN13    0.315631\n",
      "POP_DENS        0.236549\n",
      "YEAR_2020       0.211772\n",
      "NTACode_MN23    0.177180\n",
      "NTACode_MN24    0.168279\n",
      "NTACode_MN12    0.132604\n",
      "NTACode_MN40    0.107831\n",
      "NTACode_BK82    0.103225\n",
      "NTACode_BK38    0.090702\n",
      "NTACode_QN29    0.089787\n",
      "NTACode_MN15    0.089285\n",
      "NTACode_BK61    0.087792\n",
      "NTACode_MN25    0.076311\n",
      "NTACode_QN22    0.063368\n",
      "NTACode_MN14    0.061253\n",
      "NTACode_MN34    0.059944\n",
      "NTACode_MN11    0.059486\n",
      "Name: counts, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Correlations in one-hot encoded dataframe\n",
    "print(df_count2_dummy_w.corr()['counts'].abs().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idenify features correlated to the target for daily average prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          1.000000\n",
      "SUB_CNT         0.834118\n",
      "NTACode_MN17    0.671297\n",
      "NTACode_MN13    0.338276\n",
      "POP_DENS        0.253520\n",
      "NTACode_MN23    0.189892\n",
      "NTACode_MN24    0.180352\n",
      "NTACode_MN12    0.142117\n",
      "NTACode_MN40    0.115567\n",
      "NTACode_BK82    0.110630\n",
      "WEEK_53         0.101409\n",
      "NTACode_BK38    0.097209\n",
      "NTACode_QN29    0.096228\n",
      "NTACode_MN15    0.095690\n",
      "NTACode_BK61    0.094090\n",
      "NTACode_MN25    0.081785\n",
      "NTACode_QN22    0.067915\n",
      "NTACode_MN14    0.065647\n",
      "NTACode_MN34    0.064244\n",
      "NTACode_MN11    0.063754\n",
      "Name: counts, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Correlations in one-hot encoded dataframe\n",
    "print(df_count2_dummy_d.corr()['counts'].abs().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign X and y to run one of two problems (weekly total or daily avg) at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++ Predictions using Weeky total set up   +++++++++++++++++\n",
    "# X = X_w\n",
    "# y = y_w\n",
    "\n",
    "#++++++++++++++ Predictions using Daily average set up   +++++++++++++++++\n",
    "X = X_d\n",
    "y = y_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression algorithms start here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and prediction of this model (as well as others) is extremply slow! For naive base run and hyperparameter tunning we will use a subset of data. We'll use a set reasonablly large subset to make a judgment while improving the total execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First naive run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy results for KNN Regression with train data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.815\n",
      "MAE: 0.321\n",
      "RMSE: 0.442\n",
      "MSE: 0.195\n",
      "Baseline MAE: 0.757\n",
      "Baseline RMSE: 1.055 \n",
      "\n",
      "Accuracy results for KNN Regression with test data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.798\n",
      "MAE: 0.336\n",
      "RMSE: 0.465\n",
      "MSE: 0.216\n",
      "Baseline MAE: 0.763\n",
      "Baseline RMSE: 1.063\n"
     ]
    }
   ],
   "source": [
    "X1 = X[:30000,:]\n",
    "y1= y[:30000]\n",
    "# X1 = X\n",
    "# y1= y \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=50)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print('\\nAccuracy results for KNN Regression with train data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = knn.predict(X_train)\n",
    "\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_train, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_train)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_train) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_train, y_predicted))\n",
    "\n",
    "baseline = np.median(y_train)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_train)))\n",
    "print(\"Baseline RMSE: %.3f \"% np.sqrt(np.mean((baseline - y_train) ** 2)))\n",
    "\n",
    "\n",
    "print('\\nAccuracy results for KNN Regression with test data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = knn.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_test, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_test)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_test) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_test, y_predicted))\n",
    "baseline = np.median(y_test)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_test)))\n",
    "print(\"Baseline RMSE: %.3f\"% np.sqrt(np.mean((baseline - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  (50, 'distance', 2) ; Trainig R2 score: 1.000; Test R2 score: 0.799\n",
      "Parameters:  (50, 'distance', 1) ; Trainig R2 score: 1.000; Test R2 score: 0.799\n",
      "Parameters:  (50, 'uniform', 2) ; Trainig R2 score: 0.815; Test R2 score: 0.798\n",
      "Parameters:  (50, 'uniform', 1) ; Trainig R2 score: 0.815; Test R2 score: 0.798\n",
      "Parameters:  (20, 'uniform', 1) ; Trainig R2 score: 0.820; Test R2 score: 0.792\n",
      "Parameters:  (20, 'uniform', 2) ; Trainig R2 score: 0.820; Test R2 score: 0.792\n",
      "Parameters:  (20, 'distance', 1) ; Trainig R2 score: 1.000; Test R2 score: 0.792\n",
      "Parameters:  (20, 'distance', 2) ; Trainig R2 score: 1.000; Test R2 score: 0.792\n",
      "Parameters:  (10, 'uniform', 1) ; Trainig R2 score: 0.829; Test R2 score: 0.781\n",
      "Parameters:  (10, 'uniform', 2) ; Trainig R2 score: 0.829; Test R2 score: 0.781\n",
      "Parameters:  (10, 'distance', 1) ; Trainig R2 score: 1.000; Test R2 score: 0.781\n",
      "Parameters:  (10, 'distance', 2) ; Trainig R2 score: 1.000; Test R2 score: 0.781\n",
      "Parameters:  (5, 'uniform', 1) ; Trainig R2 score: 0.845; Test R2 score: 0.759\n",
      "Parameters:  (5, 'uniform', 2) ; Trainig R2 score: 0.845; Test R2 score: 0.759\n",
      "Parameters:  (5, 'distance', 1) ; Trainig R2 score: 1.000; Test R2 score: 0.759\n",
      "Parameters:  (5, 'distance', 2) ; Trainig R2 score: 1.000; Test R2 score: 0.759\n",
      "Parameters:  (3, 'distance', 2) ; Trainig R2 score: 1.000; Test R2 score: 0.729\n",
      "Parameters:  (3, 'uniform', 1) ; Trainig R2 score: 0.867; Test R2 score: 0.729\n",
      "Parameters:  (3, 'uniform', 2) ; Trainig R2 score: 0.867; Test R2 score: 0.729\n",
      "Parameters:  (3, 'distance', 1) ; Trainig R2 score: 1.000; Test R2 score: 0.729\n"
     ]
    }
   ],
   "source": [
    "X1 = X[:30000,:]\n",
    "y1= y[:30000]\n",
    "# X1 = X\n",
    "# y1= y \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "neighbors = [3,5,10,20,50]\n",
    "wghts=['uniform', 'distance']\n",
    "distance=[1,2] #1-manhattan; 2-euclidean\n",
    "\n",
    "models = {}\n",
    "for i in range(len(neighbors)):       \n",
    "    for j in range(len(wghts)):       \n",
    "        for w in range(len(distance)): \n",
    "            knn = KNeighborsRegressor(n_neighbors=neighbors[i], weights=wghts[j],p=distance[w])\n",
    "            knn.fit(X_train, y_train) \n",
    "            \n",
    "            y_predicted = knn.predict(X_train)\n",
    "            train_r2 = r2_score(y_train, y_predicted)\n",
    "            \n",
    "            y_predicted = knn.predict(X_test)\n",
    "            test_r2 = r2_score(y_test, y_predicted) \n",
    "            \n",
    "            key = [neighbors[i], wghts[j], distance[w]]\n",
    "            models[tuple(key)] = [train_r2, test_r2]\n",
    "            \n",
    "models_sorted=sorted(models.items(), key=lambda elem: elem[1][1], reverse=True)  \n",
    "     \n",
    "for i in range(len(models_sorted)):\n",
    "    print('Parameters: ', models_sorted[i][0],'; Trainig R2 score: {:.3f}; Test R2 score: {:.3f}'.format(models_sorted[i][1][0],models_sorted[i][1][1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use best model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy results for KNN Regression with train data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 1.000\n",
      "MAE: 0.000\n",
      "RMSE: 0.000\n",
      "MSE: 0.000\n",
      "Baseline MAE: 1.050\n",
      "Baseline RMSE: 2.082 \n",
      "\n",
      "Accuracy results for KNN Regression with test data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.938\n",
      "MAE: 0.326\n",
      "RMSE: 0.513\n",
      "MSE: 0.263\n",
      "Baseline MAE: 1.077\n",
      "Baseline RMSE: 2.121\n"
     ]
    }
   ],
   "source": [
    "#Use all data with best model from previous step\n",
    "X1 = X \n",
    "y1 = y \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "#++++++++++++++ Predictions best model for weekly  +++++++++++++++++\n",
    "# knn = KNeighborsRegressor(n_neighbors=20, weights='distance', p=1)\n",
    "\n",
    "#++++++++++++++ Predictions best model for daily average +++++++++++++++++\n",
    "knn = KNeighborsRegressor(n_neighbors=50, weights='distance', p=1)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print('\\nAccuracy results for KNN Regression with train data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = knn.predict(X_train)\n",
    "\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_train, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_train)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_train) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_train, y_predicted))\n",
    "\n",
    "baseline = np.median(y_train)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_train)))\n",
    "print(\"Baseline RMSE: %.3f \"% np.sqrt(np.mean((baseline - y_train) ** 2)))\n",
    "\n",
    "\n",
    "print('\\nAccuracy results for KNN Regression with test data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = knn.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_test, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_test)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_test) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_test, y_predicted))\n",
    "baseline = np.median(y_test)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_test)))\n",
    "print(\"Baseline RMSE: %.3f\"% np.sqrt(np.mean((baseline - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  ('Ridge', 0.9) ; Trainig R2 score 0.932; test R2 score: 0.935\n",
      "Parameters:  ('Ridge', 0.6) ; Trainig R2 score 0.932; test R2 score: 0.935\n",
      "Parameters:  ('Ridge', 0.3) ; Trainig R2 score 0.932; test R2 score: 0.935\n",
      "Parameters:  ('Ridge', 0.1) ; Trainig R2 score 0.932; test R2 score: 0.935\n",
      "Parameters:  ('LR', 1) ; Trainig R2 score 0.932; test R2 score: 0.935\n",
      "Parameters:  ('Lasso', 0.1) ; Trainig R2 score 0.697; test R2 score: 0.707\n",
      "Parameters:  ('Lasso', 0.3) ; Trainig R2 score 0.672; test R2 score: 0.682\n",
      "Parameters:  ('Lasso', 0.6) ; Trainig R2 score 0.605; test R2 score: 0.616\n",
      "Parameters:  ('Lasso', 0.9) ; Trainig R2 score 0.493; test R2 score: 0.503\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "models = {}\n",
    "\n",
    "LRmodel = LinearRegression()\n",
    "LRmodel.fit(X_train, y_train)       \n",
    "            \n",
    "y_predicted = LRmodel.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_predicted)           \n",
    "\n",
    "y_predicted = LRmodel.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_predicted)\n",
    "           \n",
    "key = ['LR', 1]\n",
    "models[tuple(key)] = [train_r2, test_r2]\n",
    "\n",
    "alp = [.1,.3, .6, .9]\n",
    "\n",
    "for i in range(len(alp)): \n",
    "    \n",
    "    LRmodel = linear_model.Lasso(alpha=alp[i])\n",
    "    LRmodel.fit(X_train, y_train)       \n",
    "            \n",
    "    y_predicted = LRmodel.predict(X_train)\n",
    "    train_r2 = r2_score(y_train, y_predicted)           \n",
    "    \n",
    "    y_predicted = LRmodel.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_predicted)           \n",
    "    key = ['Lasso', alp[i]]\n",
    "    models[tuple(key)] = [train_r2, test_r2]\n",
    "    \n",
    "    LRmodel = Ridge(alpha=alp[i])\n",
    "    LRmodel.fit(X_train, y_train)       \n",
    "            \n",
    "    y_predicted = LRmodel.predict(X_train)\n",
    "    train_r2 = r2_score(y_train, y_predicted)           \n",
    "    \n",
    "    y_predicted = LRmodel.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_predicted)           \n",
    "    key = ['Ridge', alp[i]]\n",
    "    models[tuple(key)] = [train_r2, test_r2]\n",
    "            \n",
    "models_sorted=sorted(models.items(), key=lambda elem: elem[1][1], reverse=True)  \n",
    "     \n",
    "for i in range(len(models_sorted)):\n",
    "    print('Parameters: ', models_sorted[i][0],'; Trainig R2 score {:.3f}; test R2 score: {:.3f}'.format(models_sorted[i][1][0],models_sorted[i][1][1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy results for Linear Regression with train data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.932\n",
      "MAE: 0.326\n",
      "RMSE: 0.529\n",
      "MSE: 0.279\n",
      "Baseline MAE: 1.050\n",
      "Baseline RMSE: 2.082 \n",
      "\n",
      "Accuracy results for Linear Regression with test data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.935\n",
      "MAE: 0.331\n",
      "RMSE: 0.526\n",
      "MSE: 0.277\n",
      "Baseline MAE: 1.077\n",
      "Baseline RMSE: 2.121\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "LRmodel = LinearRegression()\n",
    "\n",
    "LRmodel.fit(X_train, y_train)\n",
    "\n",
    "print('\\nAccuracy results for Linear Regression with train data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = LRmodel.predict(X_train)\n",
    "\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_train, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_train)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_train) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_train, y_predicted))\n",
    "\n",
    "baseline = np.median(y_train)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_train)))\n",
    "print(\"Baseline RMSE: %.3f \"% np.sqrt(np.mean((baseline - y_train) ** 2)))\n",
    "\n",
    "\n",
    "print('\\nAccuracy results for Linear Regression with test data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = LRmodel.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_test, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_test)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_test) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_test, y_predicted))\n",
    "baseline = np.median(y_test)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_test)))\n",
    "print(\"Baseline RMSE: %.3f\"% np.sqrt(np.mean((baseline - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  (100,) ; Trainig R2 score: 0.981; Test R2 score: 0.859\n",
      "Parameters:  (200,) ; Trainig R2 score: 0.982; Test R2 score: 0.859\n",
      "Parameters:  (50,) ; Trainig R2 score: 0.981; Test R2 score: 0.858\n",
      "Parameters:  (20,) ; Trainig R2 score: 0.979; Test R2 score: 0.855\n",
      "Parameters:  (10,) ; Trainig R2 score: 0.976; Test R2 score: 0.853\n"
     ]
    }
   ],
   "source": [
    "X1 = X[:20000,:]\n",
    "y1= y[:20000]\n",
    "# X1 = X\n",
    "# y1= y \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "n_trees = [10,20,50,100, 200]\n",
    "\n",
    "models = {}\n",
    "for i in range(len(n_trees)):       \n",
    "    regressor = RandomForestRegressor(n_estimators = n_trees[i], random_state = 0)\n",
    "    regressor.fit(X_train, y_train)  \n",
    "            \n",
    "    y_predicted = regressor.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_predicted) \n",
    "            \n",
    "    y_predicted = regressor.predict(X_test)\n",
    "    r2_test = r2_score(y_test, y_predicted)  \n",
    "    \n",
    "    key = [n_trees[i]]\n",
    "    models[tuple(key)] = [r2_train, r2_test]\n",
    "            \n",
    "models_sorted=sorted(models.items(), key=lambda elem: elem[1][1], reverse=True)  \n",
    "     \n",
    "for i in range(len(models_sorted)):\n",
    "    print('Parameters: ', models_sorted[i][0],'; Trainig R2 score: {:.3f}; Test R2 score: {:.3f}'.format(models_sorted[i][1][0],models_sorted[i][1][1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use best model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy results for Rendom Forest Regression with train data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.994\n",
      "MAE: 0.110\n",
      "RMSE: 0.161\n",
      "MSE: 0.026\n",
      "Baseline MAE: 1.050\n",
      "Baseline RMSE: 2.082 \n",
      "\n",
      "Accuracy results for Rendom Forest Regression with test data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.957\n",
      "MAE: 0.297\n",
      "RMSE: 0.429\n",
      "MSE: 0.184\n",
      "Baseline MAE: 1.077\n",
      "Baseline RMSE: 2.121\n"
     ]
    }
   ],
   "source": [
    "#Use best model from previous step\n",
    "X1 = X\n",
    "y1= y \n",
    "\n",
    "#++++++++++++++ Predictions best model for weekly  +++++++++++++++++\n",
    "# best_n_trees = 200\n",
    "\n",
    "#++++++++++++++ Predictions best model for daily average +++++++++++++++++\n",
    "best_n_trees = 100\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "# create regressor object and fit with train data\n",
    "regressor = RandomForestRegressor(n_estimators = best_n_trees, random_state = 0)\n",
    "regressor.fit(X_train, y_train)  \n",
    "\n",
    "print('\\nAccuracy results for Random Forest Regression with train data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = regressor.predict(X_train)\n",
    "\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_train, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_train)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_train) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_train, y_predicted))\n",
    "\n",
    "baseline = np.median(y_train)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_train)))\n",
    "print(\"Baseline RMSE: %.3f \"% np.sqrt(np.mean((baseline - y_train) ** 2)))\n",
    "\n",
    "\n",
    "print('\\nAccuracy results for Random Forest Regression with test data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = regressor.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_test, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_test)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_test) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_test, y_predicted))\n",
    "baseline = np.median(y_test)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_test)))\n",
    "print(\"Baseline RMSE: %.3f\"% np.sqrt(np.mean((baseline - y_test) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron (MLP) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  (20, 'relu', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.870; Test R2 score: 0.861\n",
      "Parameters:  (20, 'relu', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.870; Test R2 score: 0.861\n",
      "Parameters:  (50, 'relu', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.870; Test R2 score: 0.860\n",
      "Parameters:  (50, 'relu', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.870; Test R2 score: 0.860\n",
      "Parameters:  (10, 'relu', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.867; Test R2 score: 0.859\n",
      "Parameters:  (10, 'relu', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.867; Test R2 score: 0.859\n",
      "Parameters:  (10, 'logistic', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.859; Test R2 score: 0.848\n",
      "Parameters:  (10, 'logistic', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.859; Test R2 score: 0.848\n",
      "Parameters:  (20, 'logistic', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.858; Test R2 score: 0.846\n",
      "Parameters:  (20, 'logistic', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.858; Test R2 score: 0.846\n",
      "Parameters:  (50, 'logistic', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.857; Test R2 score: 0.845\n",
      "Parameters:  (50, 'logistic', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.857; Test R2 score: 0.845\n",
      "Parameters:  (50, 'tanh', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.856; Test R2 score: 0.844\n",
      "Parameters:  (50, 'tanh', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.856; Test R2 score: 0.844\n",
      "Parameters:  (10, 'tanh', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.855; Test R2 score: 0.843\n",
      "Parameters:  (10, 'tanh', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.855; Test R2 score: 0.843\n",
      "Parameters:  (20, 'tanh', 0.001, 'sgd', 0.001, 200, 'adaptive') ; Trainig R2 score: 0.855; Test R2 score: 0.843\n",
      "Parameters:  (20, 'tanh', 0.001, 'sgd', 0.001, 500, 'adaptive') ; Trainig R2 score: 0.855; Test R2 score: 0.843\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X1 = X[:20000,:]\n",
    "y1= y[:20000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "# p_hidden_layer_neurons = [3,5,10,20,30]\n",
    "# p_activation= ['logistic','tanh','relu']\n",
    "# p_alpha=[0.1, .01, .001] \n",
    "# p_learning_rate=['constant', 'adaptive']\n",
    "# p_solver=['lbfgs','sgd']\n",
    "# p_learning_rate_init=[0.01, .001, .0001]\n",
    "# p_max_iter=[200, 500, 1000]\n",
    "\n",
    "p_hidden_layer_neurons = [10,20, 50]\n",
    "p_activation= ['logistic', 'tanh','relu']\n",
    "p_alpha=[.001] \n",
    "p_learning_rate=['adaptive']\n",
    "p_solver=['sgd']\n",
    "p_learning_rate_init=[0.001]\n",
    "p_max_iter=[200, 500]\n",
    "\n",
    "models = {}\n",
    "for i in range(len(p_hidden_layer_neurons)):       \n",
    "    for j in range(len(p_activation)):       \n",
    "        for w in range(len(p_alpha)): \n",
    "            for u in range(len(p_solver)): \n",
    "                for z in range(len(p_learning_rate_init)): \n",
    "                    for v in range(len(p_max_iter)): \n",
    "                        for k in range(len(p_learning_rate)):\n",
    "                            mlpregr = MLPRegressor(random_state=42, hidden_layer_sizes=(p_hidden_layer_neurons[i], ), alpha=p_alpha[w], solver=p_solver[u],   \n",
    "                                max_iter=p_max_iter[v], learning_rate=p_learning_rate[k], activation=p_activation[j], learning_rate_init=p_learning_rate_init[z],\n",
    "                                verbose=False)\n",
    " \n",
    "                            mlpregr.fit(X_train, y_train)\n",
    "                        \n",
    "                            y_predicted = mlpregr.predict(X_train)\n",
    "                            r2_train = r2_score(y_train, y_predicted) \n",
    "                        \n",
    "                            y_predicted = mlpregr.predict(X_test)\n",
    "                            r2_test = r2_score(y_test, y_predicted) \n",
    "                        \n",
    "                            key = [p_hidden_layer_neurons[i], p_activation[j], p_alpha[w], p_solver[u],p_learning_rate_init[z],p_max_iter[v],p_learning_rate[k]]\n",
    "                            models[tuple(key)] = [r2_train, r2_test]\n",
    "            \n",
    "models_sorted=sorted(models.items(), key=lambda elem: elem[1][1], reverse=True)  \n",
    "     \n",
    "for i in range(len(models_sorted)):\n",
    "    print('Parameters: ', models_sorted[i][0],'; Trainig R2 score: {:.3f}; Test R2 score: {:.3f}'.format(models_sorted[i][1][0],models_sorted[i][1][1]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy results for MLP with train data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.957\n",
      "MAE: 0.287\n",
      "RMSE: 0.419\n",
      "MSE: 0.175\n",
      "Baseline MAE: 1.050\n",
      "Baseline RMSE: 2.082 \n",
      "\n",
      "Accuracy results for MLP with test data\n",
      "======================================================\n",
      "\n",
      "Coefficient of determination r^2 variance score : 0.959\n",
      "MAE: 0.293\n",
      "RMSE: 0.416\n",
      "MSE: 0.173\n",
      "Baseline MAE: 1.077\n",
      "Baseline RMSE: 2.121\n"
     ]
    }
   ],
   "source": [
    "X1 = X \n",
    "y1 = y \n",
    "\n",
    "#Set up with best parameters chosen from previous step\n",
    "s_alpha=.001\n",
    "s_learning_rate='adaptive'\n",
    "s_solver='sgd'\n",
    "s_learning_rate_init=0.001\n",
    "\n",
    "#++++++++++++++ Predictions best model for weekly  +++++++++++++++++\n",
    "# s_hidden_layer_neurons = 50\n",
    "# s_activation= 'logistic' \n",
    "s_max_iter=500\n",
    "\n",
    "#++++++++++++++ Predictions best model for daily average +++++++++++++++++\n",
    "s_hidden_layer_neurons = 20\n",
    "s_activation= 'relu' \n",
    "s_max_iter=200\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "MLPmodel = MLPRegressor(random_state=42, hidden_layer_sizes=(s_hidden_layer_neurons,), alpha=s_alpha, solver=s_solver,   \n",
    "        max_iter=s_max_iter, learning_rate=s_learning_rate, activation=s_activation, learning_rate_init=s_learning_rate_init,verbose=False)\n",
    " \n",
    "MLPmodel.fit(X_train, y_train)\n",
    "\n",
    "print('\\nAccuracy results for MLP with train data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = MLPmodel.predict(X_train)\n",
    "\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_train, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_train)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_train) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_train, y_predicted))\n",
    "\n",
    "baseline = np.median(y_train)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_train)))\n",
    "print(\"Baseline RMSE: %.3f \"% np.sqrt(np.mean((baseline - y_train) ** 2)))\n",
    "\n",
    "\n",
    "print('\\nAccuracy results for MLP with test data')\n",
    "print('======================================================')\n",
    "\n",
    "y_predicted = MLPmodel.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('\\nCoefficient of determination r^2 variance score : %.3f' % r2_score(y_test, y_predicted))\n",
    "print(\"MAE: %.3f\"% np.mean(abs(y_predicted - y_test)))\n",
    "print(\"RMSE: %.3f\"% np.sqrt(np.mean((y_predicted - y_test) ** 2)))\n",
    "print(\"MSE: %.3f\" % mean_squared_error(y_test, y_predicted))\n",
    "baseline = np.median(y_test)\n",
    "print(\"Baseline MAE: %.3f\"% np.mean(abs(baseline - y_test)))\n",
    "print(\"Baseline RMSE: %.3f\"% np.sqrt(np.mean((baseline - y_test) ** 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
