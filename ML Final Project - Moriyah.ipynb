{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression & ANN Models - Moriyah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/morischick/Documents/CUNY/Spring 2021/Machine Learning/final project/NYPD_Complaint_All_Clean.csv', sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6817380, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using data from 3 crime types for classification**\n",
    "\n",
    "**1. Dangerous Drugs**\n",
    "\n",
    "**2. Burglary**\n",
    "\n",
    "**3. Intoxicated & Impaired Driving**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADDR_PCT_CD</th>\n",
       "      <th>CMPLNT_FR_DT</th>\n",
       "      <th>CMPLNT_FR_TM</th>\n",
       "      <th>LAW_CAT_CD</th>\n",
       "      <th>OFNS_DESC</th>\n",
       "      <th>LOC_OF_OCCUR_DESC</th>\n",
       "      <th>JURIS_DESC</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>NTACode</th>\n",
       "      <th>...</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "      <th>TIME_CAT</th>\n",
       "      <th>INSIDE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>WEEKEND</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.0</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>06:40:00</td>\n",
       "      <td>FELONY</td>\n",
       "      <td>BURGLARY</td>\n",
       "      <td>INSIDE</td>\n",
       "      <td>N.Y. POLICE DEPT</td>\n",
       "      <td>40.705442</td>\n",
       "      <td>-73.807102</td>\n",
       "      <td>QN61</td>\n",
       "      <td>...</td>\n",
       "      <td>05:29:28</td>\n",
       "      <td>20:17:29</td>\n",
       "      <td>MORNING AFTER SUNRISE</td>\n",
       "      <td>INSIDE</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>WEEKDAY</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>32.0</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>21:45:00</td>\n",
       "      <td>MISDEMEANOR</td>\n",
       "      <td>DANGEROUS DRUGS</td>\n",
       "      <td>OPPOSITE OF</td>\n",
       "      <td>N.Y. HOUSING POLICE</td>\n",
       "      <td>40.825908</td>\n",
       "      <td>-73.937427</td>\n",
       "      <td>MN03</td>\n",
       "      <td>...</td>\n",
       "      <td>05:29:28</td>\n",
       "      <td>20:17:29</td>\n",
       "      <td>NIGHT</td>\n",
       "      <td>OUTSIDE</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>WEEKDAY</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>18:15:00</td>\n",
       "      <td>MISDEMEANOR</td>\n",
       "      <td>DANGEROUS DRUGS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N.Y. POLICE DEPT</td>\n",
       "      <td>40.622604</td>\n",
       "      <td>-73.994295</td>\n",
       "      <td>BK28</td>\n",
       "      <td>...</td>\n",
       "      <td>05:29:28</td>\n",
       "      <td>20:17:29</td>\n",
       "      <td>EVENING BEFORE SUNSET</td>\n",
       "      <td>OUTSIDE</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>WEEKDAY</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>61.0</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>20:30:00</td>\n",
       "      <td>FELONY</td>\n",
       "      <td>BURGLARY</td>\n",
       "      <td>INSIDE</td>\n",
       "      <td>N.Y. POLICE DEPT</td>\n",
       "      <td>40.586678</td>\n",
       "      <td>-73.920009</td>\n",
       "      <td>BK17</td>\n",
       "      <td>...</td>\n",
       "      <td>05:29:28</td>\n",
       "      <td>20:17:29</td>\n",
       "      <td>EVENING AFTER SUNSET</td>\n",
       "      <td>INSIDE</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>WEEKDAY</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>52.0</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>21:09:00</td>\n",
       "      <td>MISDEMEANOR</td>\n",
       "      <td>DANGEROUS DRUGS</td>\n",
       "      <td>FRONT OF</td>\n",
       "      <td>N.Y. POLICE DEPT</td>\n",
       "      <td>40.864735</td>\n",
       "      <td>-73.905231</td>\n",
       "      <td>BX30</td>\n",
       "      <td>...</td>\n",
       "      <td>05:29:28</td>\n",
       "      <td>20:17:29</td>\n",
       "      <td>NIGHT</td>\n",
       "      <td>OUTSIDE</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>WEEKDAY</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ADDR_PCT_CD CMPLNT_FR_DT CMPLNT_FR_TM   LAW_CAT_CD        OFNS_DESC  \\\n",
       "1         103.0   2015-05-28     06:40:00       FELONY         BURGLARY   \n",
       "36         32.0   2015-05-28     21:45:00  MISDEMEANOR  DANGEROUS DRUGS   \n",
       "57         62.0   2015-05-28     18:15:00  MISDEMEANOR  DANGEROUS DRUGS   \n",
       "70         61.0   2015-05-28     20:30:00       FELONY         BURGLARY   \n",
       "73         52.0   2015-05-28     21:09:00  MISDEMEANOR  DANGEROUS DRUGS   \n",
       "\n",
       "   LOC_OF_OCCUR_DESC           JURIS_DESC   Latitude  Longitude NTACode  ...  \\\n",
       "1             INSIDE     N.Y. POLICE DEPT  40.705442 -73.807102    QN61  ...   \n",
       "36       OPPOSITE OF  N.Y. HOUSING POLICE  40.825908 -73.937427    MN03  ...   \n",
       "57               NaN     N.Y. POLICE DEPT  40.622604 -73.994295    BK28  ...   \n",
       "70            INSIDE     N.Y. POLICE DEPT  40.586678 -73.920009    BK17  ...   \n",
       "73          FRONT OF     N.Y. POLICE DEPT  40.864735 -73.905231    BX30  ...   \n",
       "\n",
       "     sunrise    sunset               TIME_CAT   INSIDE TIME WEEK  DAY  \\\n",
       "1   05:29:28  20:17:29  MORNING AFTER SUNRISE   INSIDE    6   22   28   \n",
       "36  05:29:28  20:17:29                  NIGHT  OUTSIDE   21   22   28   \n",
       "57  05:29:28  20:17:29  EVENING BEFORE SUNSET  OUTSIDE   18   22   28   \n",
       "70  05:29:28  20:17:29   EVENING AFTER SUNSET   INSIDE   20   22   28   \n",
       "73  05:29:28  20:17:29                  NIGHT  OUTSIDE   21   22   28   \n",
       "\n",
       "    DAY_OF_WEEK  WEEKEND  YEAR  \n",
       "1             3  WEEKDAY  2015  \n",
       "36            3  WEEKDAY  2015  \n",
       "57            3  WEEKDAY  2015  \n",
       "70            3  WEEKDAY  2015  \n",
       "73            3  WEEKDAY  2015  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df['OFNS_DESC'].isin([\"DANGEROUS DRUGS\", \"BURGLARY\", \"INTOXICATED & IMPAIRED DRIVING\"])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728486, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction**\n",
    "\n",
    "Taking most informative attributes and discarding others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['OFNS_DESC']\n",
    "# X=df_knn[['ADDR_PCT_CD','Latitude','Longitude','YEAR','WEEK','DAY','DAY_OF_WEEK', 'TIME', 'TIME_CAT', 'INSIDE']]\n",
    "# X=df_knn[['ADDR_PCT_CD','YEAR','WEEK','DAY','DAY_OF_WEEK', 'TIME', 'TIME_CAT', 'INSIDE']]\n",
    "\n",
    "X=df[['WEEK','DAY','DAY_OF_WEEK', 'TIME', 'TIME_CAT', 'INSIDE']]\n",
    "X_ready = pd.get_dummies(X, columns=['WEEK','DAY','DAY_OF_WEEK', 'TIME', 'TIME_CAT', 'INSIDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728486, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling the data for use in logistic regression and MLP models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_ready)\n",
    "std_data = scaler.transform(X_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(std_data, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning for MLP Model**\n",
    "\n",
    "First attribute will be hidden layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer Sizes= (1,)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic:  0.7327897431673737\n",
      "ReLu:  0.7599967055141457\n",
      "Tanh:  0.7321033919477274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "logistic_mlp = MLPClassifier(hidden_layer_sizes=(1,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'logistic', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "logistic_mlp.fit(X_train, y_train)\n",
    "logistic_mlp.predict(X_test)\n",
    "logistic_score = logistic_mlp.score(X_test, y_test)\n",
    "print(\"Logistic: \", logistic_score)\n",
    "\n",
    "relu_mlp = MLPClassifier(hidden_layer_sizes=(1,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'relu', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "relu_mlp.fit(X_train, y_train)\n",
    "relu_mlp.predict(X_test)\n",
    "relu_score = relu_mlp.score(X_test, y_test)\n",
    "print(\"ReLu: \", relu_score)\n",
    "\n",
    "tanh_mlp = MLPClassifier(hidden_layer_sizes=(1,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'tanh', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "tanh_mlp.fit(X_train, y_train)\n",
    "tanh_mlp.predict(X_test)\n",
    "tanh_score = tanh_mlp.score(X_test, y_test)\n",
    "print(\"Tanh: \", tanh_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer Sizes=(2,2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic:  0.772666749028813\n",
      "ReLu:  0.7713626817114854\n",
      "Tanh:  0.7696742577111559\n"
     ]
    }
   ],
   "source": [
    "logistic_mlp = MLPClassifier(hidden_layer_sizes=(2,2), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'logistic', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "logistic_mlp.fit(X_train, y_train)\n",
    "logistic_mlp.predict(X_test)\n",
    "logistic_score = logistic_mlp.score(X_test, y_test)\n",
    "print(\"Logistic: \", logistic_score)\n",
    "\n",
    "relu_mlp = MLPClassifier(hidden_layer_sizes=(2,2), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'relu', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "relu_mlp.fit(X_train, y_train)\n",
    "relu_mlp.predict(X_test)\n",
    "relu_score = relu_mlp.score(X_test, y_test)\n",
    "print(\"ReLu: \", relu_score)\n",
    "\n",
    "tanh_mlp = MLPClassifier(hidden_layer_sizes=(2,2), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'tanh', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "tanh_mlp.fit(X_train, y_train)\n",
    "tanh_mlp.predict(X_test)\n",
    "tanh_score = tanh_mlp.score(X_test, y_test)\n",
    "print(\"Tanh: \", tanh_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer Sizes=(10,)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic:  0.7742453568339991\n",
      "ReLu:  0.7740874960534805\n",
      "Tanh:  0.7735315515655671\n"
     ]
    }
   ],
   "source": [
    "logistic_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'logistic', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "logistic_mlp.fit(X_train, y_train)\n",
    "logistic_mlp.predict(X_test)\n",
    "logistic_score = logistic_mlp.score(X_test, y_test)\n",
    "print(\"Logistic: \", logistic_score)\n",
    "\n",
    "relu_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'relu', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "relu_mlp.fit(X_train, y_train)\n",
    "relu_mlp.predict(X_test)\n",
    "relu_score = relu_mlp.score(X_test, y_test)\n",
    "print(\"ReLu: \", relu_score)\n",
    "\n",
    "tanh_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'tanh', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "tanh_mlp.fit(X_train, y_train)\n",
    "tanh_mlp.predict(X_test)\n",
    "tanh_score = tanh_mlp.score(X_test, y_test)\n",
    "print(\"Tanh: \", tanh_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Layer Sizes= (50,)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic:  0.7743963541023212\n",
      "ReLu:  0.7739364987851584\n",
      "Tanh:  0.7735864596631388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "logistic_mlp = MLPClassifier(hidden_layer_sizes=(50,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'logistic', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "logistic_mlp.fit(X_train, y_train)\n",
    "logistic_mlp.predict(X_test)\n",
    "logistic_score = logistic_mlp.score(X_test, y_test)\n",
    "print(\"Logistic: \", logistic_score)\n",
    "\n",
    "relu_mlp = MLPClassifier(hidden_layer_sizes=(50,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'relu', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "relu_mlp.fit(X_train, y_train)\n",
    "relu_mlp.predict(X_test)\n",
    "relu_score = relu_mlp.score(X_test, y_test)\n",
    "print(\"ReLu: \", relu_score)\n",
    "\n",
    "tanh_mlp = MLPClassifier(hidden_layer_sizes=(50,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'tanh', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "tanh_mlp.fit(X_train, y_train)\n",
    "tanh_mlp.predict(X_test)\n",
    "tanh_score = tanh_mlp.score(X_test, y_test)\n",
    "print(\"Tanh: \", tanh_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like hidden layer size doesn't make that much of a difference after a certain point so we'll keep hidden layer size at (10,) and check other parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next feature to be optimized is the initialized learning rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate Init=0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic:  0.7720490329311315\n",
      "ReLu:  0.771685266784719\n",
      "Tanh:  0.7725157517604909\n"
     ]
    }
   ],
   "source": [
    "logistic_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'logistic', alpha= 0.1, learning_rate_init= 0.1)\n",
    "\n",
    "logistic_mlp.fit(X_train, y_train)\n",
    "logistic_mlp.predict(X_test)\n",
    "logistic_score = logistic_mlp.score(X_test, y_test)\n",
    "print(\"Logistic: \", logistic_score)\n",
    "\n",
    "relu_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'relu', alpha= 0.1, learning_rate_init= 0.1)\n",
    "\n",
    "relu_mlp.fit(X_train, y_train)\n",
    "relu_mlp.predict(X_test)\n",
    "relu_score = relu_mlp.score(X_test, y_test)\n",
    "print(\"ReLu: \", relu_score)\n",
    "\n",
    "tanh_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'tanh', alpha= 0.1, learning_rate_init= 0.1)\n",
    "\n",
    "tanh_mlp.fit(X_train, y_train)\n",
    "tanh_mlp.predict(X_test)\n",
    "tanh_score = tanh_mlp.score(X_test, y_test)\n",
    "print(\"Tanh: \", tanh_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate Init=0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic:  0.7737305934192645\n",
      "ReLu:  0.773812955565622\n",
      "Tanh:  0.7733874178094414\n"
     ]
    }
   ],
   "source": [
    "logistic_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'logistic', alpha= 0.001, learning_rate_init= 0.001)\n",
    "\n",
    "logistic_mlp.fit(X_train, y_train)\n",
    "logistic_mlp.predict(X_test)\n",
    "logistic_score = logistic_mlp.score(X_test, y_test)\n",
    "print(\"Logistic: \", logistic_score)\n",
    "\n",
    "relu_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'relu', alpha= 0.001, learning_rate_init= 0.001)\n",
    "\n",
    "relu_mlp.fit(X_train, y_train)\n",
    "relu_mlp.predict(X_test)\n",
    "relu_score = relu_mlp.score(X_test, y_test)\n",
    "print(\"ReLu: \", relu_score)\n",
    "\n",
    "tanh_mlp = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                              activation= 'tanh', alpha= 0.001, learning_rate_init= 0.001)\n",
    "\n",
    "tanh_mlp.fit(X_train, y_train)\n",
    "tanh_mlp.predict(X_test)\n",
    "tanh_score = tanh_mlp.score(X_test, y_test)\n",
    "print(\"Tanh: \", tanh_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP w/ LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and test sets transformed using LDA\n",
    "\n",
    "Will use for both MLP and LR model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(std_data, y, test_size=0.2, random_state=1)\n",
    "\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train2, y_train2)\n",
    "X_test_lda = lda.fit_transform(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP w/ LDA:  0.7710538236626446\n"
     ]
    }
   ],
   "source": [
    "# put MLP trained on LDA transformed data here\n",
    "\n",
    "MLP_LDA = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                        activation= 'tanh', alpha= 0.01, learning_rate_init= 0.01)\n",
    "\n",
    "MLP_LDA.fit(X_train_lda, y_train2)\n",
    "MLP_LDA.predict(X_test_lda)\n",
    "MLP_LDA_score = MLP_LDA.score(X_test_lda, y_test2)\n",
    "print(\"MLP w/ LDA: \", MLP_LDA_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The optimal MLP model found:**\n",
    "\n",
    "Hidden Layer Size: (10,)\n",
    "\n",
    "Activation Function: Tanh\n",
    "\n",
    "Initialized Learning Rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7739021812241761\n"
     ]
    }
   ],
   "source": [
    "best_MLP = MLPClassifier(hidden_layer_sizes=(10,), solver= 'sgd', early_stopping= True,\n",
    "                        activation= 'tanh', alpha= 0.01, learning_rate_init= 0.01)\n",
    "best_MLP.fit(X_train, y_train)\n",
    "MLP_pred = best_MLP.predict(X_test)\n",
    "print(best_MLP.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of LR 5-Fold Cross Validation:  0.7733172226361138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "MLP_cross_val_score = cross_val_score(best_MLP, X_train, y_train, scoring='accuracy', cv=5)\n",
    "\n",
    "print(\"Mean of LR 5-Fold Cross Validation: \", np.mean(MLP_cross_val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best MLP Accuracy:  0.7739021812241761\n",
      "\n",
      "Best MLP Confusion Matrix:\n",
      "[[36341  7364   550]\n",
      " [11946 66102  4680]\n",
      " [  128  8274 10313]]\n",
      "\n",
      "Best MLP Precision = 0.773902\n",
      "Best MLP Recall = 0.773902\n",
      "Best MLP F1 Score = 0.773902\n",
      "\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      BURGLARY       0.75      0.82      0.78     44255\n",
      "               DANGEROUS DRUGS       0.81      0.80      0.80     82728\n",
      "INTOXICATED & IMPAIRED DRIVING       0.66      0.55      0.60     18715\n",
      "\n",
      "                      accuracy                           0.77    145698\n",
      "                     macro avg       0.74      0.72      0.73    145698\n",
      "                  weighted avg       0.77      0.77      0.77    145698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "accuracy_MLP = np.mean(MLP_pred == y_test)\n",
    "print(\"\\nBest MLP Accuracy: \", accuracy_MLP)\n",
    "\n",
    "\n",
    "print(\"\\nBest MLP Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, MLP_pred))\n",
    "\n",
    "\n",
    "precision_MLP = precision_score(y_test, MLP_pred, average='micro') \n",
    "print(\"\\nBest MLP Precision = %f\" % precision_MLP)\n",
    "\n",
    "recall_MLP = recall_score(y_test, MLP_pred, average='micro')\n",
    "print(\"Best MLP Recall = %f\" % recall_MLP)\n",
    "\n",
    "\n",
    "f1_MLP = f1_score(y_test, MLP_pred, average='micro')\n",
    "print(\"Best MLP F1 Score = %f\" % f1_MLP)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, MLP_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same dataset as above so data has aleady been scaled.\n",
    "\n",
    "Hyperparameter tuning of regularization constant for logistic regression model:\n",
    "\n",
    "(set by default to L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "LR_search = GridSearchCV(LogisticRegression(verbose=10, multi_class='multinomial', solver='lbfgs'), cv=5, param_grid=param_grid_)\n",
    "\n",
    "LR_search.fit(X_train, y_train)\n",
    "\n",
    "params_optimal_LR = LR_search.best_params_\n",
    "\n",
    "print(params_optimal_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728108827849387"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_pred = LR_search.predict(X_test)\n",
    "LR_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7728108827849387\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=10, multi_class='multinomial', solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "LR_pred = logreg.predict(X_test)\n",
    "print(logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of LR 5-Fold Cross Validation:  0.7725622306415109\n"
     ]
    }
   ],
   "source": [
    "LR_cross_val_score = cross_val_score(logreg, X_train, y_train, scoring='accuracy', cv=5)\n",
    "\n",
    "print(\"Mean of LR 5-Fold Cross Validation: \", np.mean(LR_cross_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression w/ LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7662905461983006\n"
     ]
    }
   ],
   "source": [
    "LR_LDA = LogisticRegression(C=10).fit(X_train_lda, y_train2)\n",
    "LR_LDA_score = LR_LDA.score(X_test_lda, y_test2)\n",
    "print(LR_LDA_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Results of Optimized Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy:  0.7728108827849387\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[36580  7168   507]\n",
      " [12198 65904  4626]\n",
      " [  110  8492 10113]]\n",
      "\n",
      "Train Precision = 0.772811\n",
      "Test Recall = 0.772811\n",
      "Test F1 Score = 0.772811\n",
      "\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      BURGLARY       0.75      0.83      0.79     44255\n",
      "               DANGEROUS DRUGS       0.81      0.80      0.80     82728\n",
      "INTOXICATED & IMPAIRED DRIVING       0.66      0.54      0.60     18715\n",
      "\n",
      "                      accuracy                           0.77    145698\n",
      "                     macro avg       0.74      0.72      0.73    145698\n",
      "                  weighted avg       0.77      0.77      0.77    145698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_LR = np.mean(LR_pred == y_test)\n",
    "print(\"\\nTrain Accuracy: \", accuracy_LR)\n",
    "\n",
    "\n",
    "print(\"\\nTrain Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, LR_pred))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, LR_pred, average='micro') \n",
    "print(\"\\nTrain Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, LR_pred, average='micro')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, LR_pred, average='micro')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, LR_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
